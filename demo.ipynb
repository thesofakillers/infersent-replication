{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69513c8-123d-436a-9c25-10e7c47a85d0",
   "metadata": {},
   "source": [
    "# InferSent - Demonstration\n",
    "\n",
    "This notebook serves as a demonstration of the usage of a trained InferSent model, as well as training and performance results on the original SNLI task and on SentEval. The notebook also provides some error analysis (discussion) on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cae2b-e857-43f8-b2ab-347d6803d5aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "The notebook assumes the user has access to the pretrained models, logs and evaluation results provided via the [archive.org](https://archive.org) link specified in `README.md`. The notebook also assumes the user has run the `data.py` script with its default arguments such that aligned glove embeddings are available.\n",
    "\n",
    "First we define our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05433cd-4077-4266-bc77-01edc90763b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from models.infersent import InferSent\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f988b-afe9-4f01-9d14-855e18133a78",
   "metadata": {},
   "source": [
    "We continue by defining a configuration dictionary that we can access throughout the demo. Of course we can change the values of our config accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67682d94-0775-4239-8bbb-d58374db7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_config = {\n",
    "    # one of 'baseline', 'lstm', 'bilstm', 'maxpoolbilstm'\n",
    "    \"encoder_type\": \"maxpoolbilstm\",\n",
    "    # path to model checkpoint. by default we load the maxpoolbilstm\n",
    "    \"checkpoint_path\": \"logs/maxpoolbilstm/version_3/checkpoints/epoch=6-step=60088.ckpt\",\n",
    "    # random seed for reproducibility\n",
    "    \"seed\": 42,\n",
    "    # where to load aligned GloVe torch tensor from\n",
    "    \"aligned_glove\": \"data/aligned_glove.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24eef4-574f-4f12-90e0-bc114be13c29",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "We can instantiate the model we will use from the checkpoint path we defined. Because the GloVe embeddings are not saved in the model, after instatiating our model we load them into it in a separate call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796dd346-2412-4ee6-9614-3c461d55ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pretrained model\n",
    "infsent_model = InferSent.load_from_checkpoint(demo_config[\"checkpoint_path\"])\n",
    "# load aligned GloVe embeddings from disk\n",
    "aligned_glove = torch.load(demo_config[\"aligned_glove\"])\n",
    "# and load them into our model\n",
    "infsent_model.load_embeddings(aligned_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430e578-0f67-46ac-b8cd-23358a3f5487",
   "metadata": {},
   "source": [
    "## Inference examples\n",
    "\n",
    "The InferSent model is trained to perform Natural Language Inference (NLI), i.e. predicting whether a \"hypothesis\" sentence is true (\"entailment\"), not true (\"contradiction\") or neither (\"neutral\") given a \"premise\" sentence.\n",
    "\n",
    "This task is a \"proxy\" task used for training an encoder sub-model that can be used to embed sentences.\n",
    "\n",
    "Below, we will first see how we can use the model for performing NLI and then how we can use its encoder to obtain an embedding (vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43eb86-d79e-4792-865a-d6fc0f9dab2e",
   "metadata": {},
   "source": [
    "### NLI\n",
    "\n",
    "Here is our model predicting the entailment between a few pairs of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ca43d8-ad26-43fe-ac9d-351e78b08525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, a quick helper function for printing\n",
    "def print_entailment_res(premise, hypothesis, result):\n",
    "    print(f\"premise: {premise}\")\n",
    "    print(f\"hypothesis: {hypothesis}\")\n",
    "    print(f\"model prediction: {result}\")\n",
    "    print(\"------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43abe6b4-e239-48a5-ab9e-69e594a1a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise: A concave lens is thinner in the middle than it is near its edges.\n",
      "hypothesis: A concave lens is thicker at the edges than it is in the middle.\n",
      "model prediction: entailment\n",
      "------------\n",
      "\n",
      "premise: but that takes too much planning\n",
      "hypothesis: It doesn't take much planning.\n",
      "model prediction: contradiction\n",
      "------------\n",
      "\n",
      "premise: Energy, heat and/or sound are forms of matter\n",
      "hypothesis: Heat, light, and sound are all different forms of energy.\n",
      "model prediction: neutral\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# entailment example from SciTail\n",
    "premise_1 = \"A concave lens is thinner in the middle than it is near its edges.\"\n",
    "hypothesis_1 = \"A concave lens is thicker at the edges than it is in the middle.\"\n",
    "result_1 = infsent_model.predict(premise_1, hypothesis_1)\n",
    "print_entailment_res(premise_1, hypothesis_1, result_1)\n",
    "\n",
    "# contradiction example from multiNLI\n",
    "premise_2 = \"but that takes too much planning\"\n",
    "hypothesis_2 = \"It doesn't take much planning.\"\n",
    "result_2 = infsent_model.predict(premise_2, hypothesis_2)\n",
    "print_entailment_res(premise_2, hypothesis_2, result_2)\n",
    "\n",
    "# neutral example from SciTail\n",
    "premise_3 = \"Energy, heat and/or sound are forms of matter\"\n",
    "hypothesis_3 = \"Heat, light, and sound are all different forms of energy.\"\n",
    "result_3 = infsent_model.predict(premise_3, hypothesis_3)\n",
    "print_entailment_res(premise_3, hypothesis_3, result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd78b96-74d9-4709-87e4-8d5ca0e21724",
   "metadata": {},
   "source": [
    "Don't trust that those weren't just hard-coded in the model? Try with some sentences of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383affa7-3c40-4e28-bba3-9fdf56185ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise: Replace this string with your premise.\n",
      "hypothesis: Replace this string with your hypothesis.\n",
      "model prediction: neutral\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace these strings and rerun this cell\n",
    "premise_yours = \"Replace this string with your premise.\"\n",
    "hypothesis_yours = \"Replace this string with your hypothesis.\"\n",
    "result_yours = infsent_model.predict(premise_yours, hypothesis_yours)\n",
    "\n",
    "print_entailment_res(premise_yours, hypothesis_yours, result_yours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6a27f-828c-49b6-882d-31f8823a5565",
   "metadata": {},
   "source": [
    "### Sentence Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea189623-229e-4a4e-98df-8334d4ef0477",
   "metadata": {},
   "source": [
    "Here is our model encoding a few sentences. Of course, just embedding a sentence is quite useless if you don't do anything with the embedding afterwards, but this is just a quick demonstration which can be extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39568439-6039-4c16-ba7f-775b7dd29c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first a helper function\n",
    "def visualize_embedding(sentence, embedding, ylim=None):\n",
    "    print(embedding)\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    plt.title(f\"'{sentence}'\")\n",
    "    plt.xlabel(\"dimension\")\n",
    "    plt.bar(np.arange(len(embedding)), embedding.cpu().numpy())\n",
    "    plt.tight_layout()\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e1d39-448d-4516-bd8a-513d370ac649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0513, -0.0105,  0.0309,  ...,  0.0399, -0.0280,  0.1223])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tensor([ 0.0546, -0.0350, -0.0233,  ...,  0.0780,  0.1136, -0.0037])\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"Hello there, here's an example sentence\"\n",
    "embedding_1 = infsent_model.encoder.encode(sentence_1).squeeze()\n",
    "visualize_embedding(sentence_1, embedding_1)\n",
    "\n",
    "sentence_2 = \"I am having a great day today\"\n",
    "embedding_2 = infsent_model.encoder.encode(sentence_2).squeeze()\n",
    "visualize_embedding(sentence_2, embedding_2)\n",
    "\n",
    "sentence_3 = \"Advanced Topics in Computational Semantics is the best course ever\"\n",
    "embedding_3 = infsent_model.encoder.encode(sentence_3).squeeze()\n",
    "visualize_embedding(sentence_3, embedding_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c63ab-96cf-4436-bbf6-bae37205658c",
   "metadata": {},
   "source": [
    "Try it with your own sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e1326-446d-4896-8874-1b7f3bb5c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_yours = \"Replace this string with your own sentence and rerun this cell.\"\n",
    "embedding_yours = infsent_model.encoder.encode(sentence_yours).squeeze()\n",
    "visualize_embedding(sentence_yours, embedding_yours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f124ff2-b016-44c9-b552-3513a4659aeb",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb94ff-77af-4f99-b296-e19a93edf71a",
   "metadata": {},
   "source": [
    "### Training/Validation Loss/Accuracy Curves\n",
    "\n",
    "Training/Validation loss and accuracy were logged in a tensorboard-compatible\n",
    "format. We can load tensorboard directly in the notebook and view them. Users\n",
    "are suggested to do the following:\n",
    "\n",
    "1. change to the `TIME SERIES` tab on the top,\n",
    "2. pin the `train_acc`, `train_loss` `val_acc`, `val_loss` metrics\n",
    "3. activate all the runs on the left hand side except `maxpoolbilstm/version_1`,\n",
    "   as these are the runs corresponding to the best model for each encoder type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a964749-07cf-4ba0-8ba5-c1d857c752b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2aaca-0742-4dc5-9cb3-4401a53c60dd",
   "metadata": {},
   "source": [
    "A few observations can be made with regards to the learning curves. First, from the validation accuracy curves we can see a clear difference in performance between the baseline model and the LSTM models, with the Max-pooled BiLSTM achieving the highest validation accuracy throughout. From these curves it is interesting to note the similarity in performance between the LSTM and the 'plain' BiLSTM, which actually does marginally worse albeit training for slightly less epochs. It is unclear why this occurs, especially since the BiLSTM contains the same information as the LSTM and could just learn to discard the additional concatenated information from the reversed LSTM. This may be explained by overfitting, with further evidence from the BiLSTM training accuracy being higher than the LSTM. This, combined with the much slower training time (2.05 hours vs 3.895 hours), indicates that the 'plain' BiLSTM is a worse choice than the LSTM, at least when it comes to SNLI (we will see and discuss SentEval performance later).\n",
    "\n",
    "On the subject of training time, we can gather more insights by hovering over the curves. While we see that the implemented early stopping criterion (stop when learning rate goes under $10^{-5}$) causes all models to end training between 12-15 epochs, the underlying run time is quite different, ranging from 26 minutes for the baseline model to 3.992 hours for the MaxPoolBiLSTM model. This range can be appreciated by changing the x-axis to relative.\n",
    "\n",
    "One may argue that the stopping criterion outlined in the paper was a bit too lenient and users may have benefited time-wise from more informed criterions. The behaviour of the validation loss curves in the LSTM-based models, with an initial dip followed by a slow rise may have been useful, particularly in combination with the validation accuracy curves. Perhaps the authors intuited that longer training could lead to better generalized sentence encoders at the expense of slightly worse NLI performance. This is similar reasoning as for why the same encoder is used for hypothesis and premise as opposed to using two separate specialized encoders.\n",
    "\n",
    "One final observation can be made by activating only the MaxPoolBiLSTM runs (version 1 and 3). Here, version 1 is an \"incorrect\" implementation of the MaxPoolBiLSTM described by the authors, where the padded values of 0 hidden states the LSTM are not masked out as they should be, to avoid incorrectly selecting them when max-pooling hiddenstates with only negative values. Despite this incorrectness, this version seems to outperform the \"correct\" implementation in terms of validation accuracy. One possible explanation for this is that not masking out the 0's somewhat simulates the application of a ReLU non-linearity, which generally makes networks more expressive. Another explanation can be that the zeros could mimic some kind of frankensteined dropout. Regardless, the difference in performance is somewhat marginal and for the moment devoid of an estimated uncertainty, making it difficult to rigorously compare. Because we were interesting in replicating parts of the original work, the `version_1` was disregarded for the rest of the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951c1cc-6eb7-4318-a6e0-a6e4ca85d5a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdf907-3d43-4d34-911f-f9a85271c209",
   "metadata": {},
   "source": [
    "We've so far examined what our models can do, and how they behaved while training. How do they do in testing? For this, we evaluate our models both on the original SNLI task, as well as the SentEval sentence embedding evaluation suite, particularly the 'MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC', and 'SICKEntailment' transfer tasks. This is done via `eval.py`. After some parsing of the results, we report \"micro\" and \"macro\" validation set accuracy on the transfer tasks, where \"macro\" refers to the classical average accuracy across the tasks while \"micro\" refers to the average accuracy across the tasks weighted by the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade0094-1871-441d-b1be-7ad6d6f60cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setup datastructures\n",
    "senteval_tasks = {\"MR\", \"CR\", \"SUBJ\", \"MPQA\", \"SST2\", \"TREC\", \"MRPC\", \"SICKEntailment\"}\n",
    "eval_data = {\n",
    "    \"baseline\": {\n",
    "        \"name\": \"Baseline\",\n",
    "        \"dim\": 300,\n",
    "        \"snli\": {},\n",
    "        \"senteval\": {},\n",
    "    },\n",
    "    \"lstm\": {\n",
    "        \"name\": \"LSTM\",\n",
    "        \"dim\": 2048,\n",
    "        \"snli\": {},\n",
    "        \"senteval\": {},\n",
    "    },\n",
    "    \"bilstm\": {\n",
    "        \"name\": \"BiLSTM\",\n",
    "        \"dim\": 4096,\n",
    "        \"snli\": {},\n",
    "        \"senteval\": {},\n",
    "    },\n",
    "    \"maxpoolbilstm\": {\n",
    "        \"name\": \"BiLSTM-Max\",\n",
    "        \"dim\": 4096,\n",
    "        \"snli\": {},\n",
    "        \"senteval\": {},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91c27c-ea59-454e-83ce-d8bdf842119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in eval_data.keys():\n",
    "    with open(f\"logs/{model}/eval/snli/val.pkl\", \"rb\") as f:\n",
    "        eval_data[model][\"snli\"][\"val\"] = pickle.load(f)[0][\"val_acc\"]\n",
    "    with open(f\"logs/{model}/eval/snli/test.pkl\", \"rb\") as f:\n",
    "        eval_data[model][\"snli\"][\"test\"] = pickle.load(f)[0][\"test_acc\"]\n",
    "    with open(f\"logs/{model}/eval/senteval/results.pkl\", \"rb\") as f:\n",
    "        eval_data[model][\"senteval\"] = {\n",
    "            k: v for k, v in pickle.load(f).items() if k in senteval_tasks\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cbf98-0c6c-45b5-b89b-752f8660e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dfs = {}\n",
    "\n",
    "# the models\n",
    "model_keys = eval_data.keys()\n",
    "# the sum of total dev samples in SentEval across the tasks\n",
    "senteval_ndev_sum = sum(\n",
    "    [eval_data[\"baseline\"][\"senteval\"][task][\"ndev\"] for task in senteval_tasks]\n",
    ")\n",
    "\n",
    "# dimensionality of embedding\n",
    "sub_dfs[\"dim\"] = pd.DataFrame(\n",
    "    columns=[\"Model\", \"\"],\n",
    "    data={\n",
    "        \"Model\": [eval_data[model][\"name\"] for model in model_keys],\n",
    "        \"\": [eval_data[model][\"dim\"] for model in model_keys],\n",
    "    },\n",
    ").set_index(\"Model\")\n",
    "\n",
    "# NLI test and val accuracy\n",
    "sub_dfs[\"NLI\"] = pd.DataFrame(\n",
    "    columns=[\"Model\", \"dev\", \"test\"],\n",
    "    data={\n",
    "        \"Model\": [eval_data[model][\"name\"] for model in model_keys],\n",
    "        \"dev\": [eval_data[model][\"snli\"][\"val\"] * 100 for model in model_keys],\n",
    "        \"test\": [eval_data[model][\"snli\"][\"test\"] * 100 for model in model_keys],\n",
    "    },\n",
    ").set_index(\"Model\")\n",
    "\n",
    "# Transfer micro and macro val accuracy\n",
    "sub_dfs[\"Transfer\"] = pd.DataFrame(\n",
    "    columns=[\"Model\", \"micro\", \"macro\"],\n",
    "    data={\n",
    "        \"Model\": [eval_data[model][\"name\"] for model in eval_data.keys()],\n",
    "        \"micro\": [\n",
    "            np.sum(\n",
    "                [\n",
    "                    eval_data[model][\"senteval\"][task][\"devacc\"]\n",
    "                    * ((eval_data[model][\"senteval\"][task][\"ndev\"]) / senteval_ndev_sum)\n",
    "                    for task in senteval_tasks\n",
    "                ]\n",
    "            )\n",
    "            for model in model_keys\n",
    "        ],\n",
    "        \"macro\": [\n",
    "            np.mean(\n",
    "                [\n",
    "                    eval_data[model][\"senteval\"][task][\"devacc\"]\n",
    "                    for task in senteval_tasks\n",
    "                ]\n",
    "            )\n",
    "            for model in model_keys\n",
    "        ],\n",
    "    },\n",
    ").set_index(\"Model\")\n",
    "\n",
    "eval_df = pd.concat(sub_dfs, axis=1)\n",
    "\n",
    "# highlight the maximum in each column (except the dim column)\n",
    "def highlight_max(x):\n",
    "    if \"dim\" not in x.name:\n",
    "        return [\"font-weight: bold; precision:1\" if v == x.max() else \"\" for v in x]\n",
    "    else:\n",
    "        return [\"\" for v in x]\n",
    "# eval_df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3302c15-3f5e-45cf-aa0c-8247237d0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.style.apply(highlight_max).format(precision=1).set_caption(\n",
    "    \"Table 1: Partial Replication of Table 3 of Conneau et al. (2017)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfb249-8639-454d-9609-2cda1070744f",
   "metadata": {},
   "source": [
    "Table 1 shows the validation (a.k.a. \"dev\") and test accuracy of the four models implemented in this repository on the NLI task, as evaluated on the SNLI dataset. It also shows the micro and macro validation accuracy across the SentEval tasks outlined above. A few remarks can be made.\n",
    "\n",
    "Firstly, we see that the trends from validation accuracy on SNLI are mirrored in the test accuracy, so the discussion from the validation curves above holds. With regards to replication, for the LSTM and BiLSTM-Max model test accuracy we are decently close to the original results of the authors.\n",
    "\n",
    "What is perhaps more interesting however is how the models perform micro- and macro-wise on the SentEval transfer tasks. We see that while the BiLSTM-Max model still achieves the highest performance, the range across the various architectures is now much more compact, ranging from 79.0 to 80.7 macro-wise and 77.2 to 81.2 micro-wise. This seems to suggest that the underlying force dominating sentence-embedding performance is some aspect of the architecture that is shared across all variants. This theory may explain why the Baseline model, almost entirely based on the GloVe word-embeddings used in all models, performs so comparatively well in this case, coming second only to the best model both micro- and macro-wise.\n",
    "\n",
    "If this were indeed the case, it would suggest that word-order is not properly learned in the LSTM models, despite the sequential nature of their learning. We can verify this by trying two sentences whose meanings are opposite but words are the same. If word order matters, the resulting word-embeddings should differ, i.e. the difference should be non-zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42b944-9a3a-4081-a33c-d18702597c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sent = \"bad, not good\"\n",
    "neg_emb = infsent_model.encoder.encode(neg_sent).squeeze()\n",
    "visualize_embedding(neg_sent, neg_emb, [-0.15, 0.275])\n",
    "\n",
    "\n",
    "pos_sent = \"good, not bad\"\n",
    "pos_emb = infsent_model.encoder.encode(pos_sent).squeeze()\n",
    "visualize_embedding(pos_sent, pos_emb, [-0.15, 0.275])\n",
    "\n",
    "diff_emb = neg_emb - pos_emb\n",
    "visualize_embedding(\n",
    "    \"Difference between 'bad, not good' and 'good, not bad'\", diff_emb, [-0.15, 0.275]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef34d1-d5e8-48dd-a553-50a929f529bc",
   "metadata": {},
   "source": [
    "While the difference is indeed non-zero, we do note that the embeddings are quite similar despite the sentences having opposite meanings. This can be interpreted as word order not fully being exploited. \n",
    "\n",
    "This may be a limit of the models employed, which at best visit sentences sequentially in both directions, but cannot examine the sentences as a graph to construct inner representations akin to a parse tree, where word order becomes increasingly useful. At the expense of triteness, it would be interesting to extend the InferSent architecture with a transformer, BERT-like encoder for the sentences, to examine whether self-attention could be leveraged for improved SentEval performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
